import pandas as pd
import numpy as np
import xgboost as xgb
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
import joblib # Used for saving/loading the scikit-learn compatible model

# 1. Load Data
df = pd.read_csv('data/kepler_koi_data.csv')

# 2. Define Features and Target
FEATURES = ['koi_period', 'koi_duration', 'koi_depth', 'koi_impact', 
            'koi_model_snr', 'koi_steff', 'koi_srad']
TARGET = 'koi_disposition'

# 3. Filter for Binary Classification (Confirmed vs. False Positive)
# This is a critical hackathon choice to maximize accuracy
df_filtered = df[df[TARGET].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()

# 4. Handle Missing Values (Simple Imputation/Dropping for quick model)
# Drop rows where any of the key 7 features are missing
df_clean = df_filtered.dropna(subset=FEATURES).copy()

# 5. Encode Target Variable (0: False Positive, 1: Confirmed)
df_clean['is_exoplanet'] = df_clean[TARGET].apply(
    lambda x: 1 if x == 'CONFIRMED' else 0
)

X = df_clean[FEATURES]
y = df_clean['is_exoplanet']

# 6. Train/Test Split
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42, stratify=y
)

# 7. Train XGBoost Model
# Use a simple set of parameters suitable for rapid training
model = xgb.XGBClassifier(
    objective='binary:logistic',
    n_estimators=100, 
    learning_rate=0.1, 
    use_label_encoder=False, 
    eval_metric='logloss',
    random_state=42
)
model.fit(X_train, y_train)

# 8. Evaluate Model (for your records and the Model Performance tab)
y_pred = model.predict(X_test)
print(f"Accuracy: {accuracy_score(y_test, y_pred):.4f}")
print("\nClassification Report:\n", classification_report(y_test, y_pred))

# 9. Save the Trained Model
# Using the native XGBoost method for saving
model.save_model('models/xgb_model.json') 

print("Model saved successfully to models/xgb_model.json")
